---
title: Insert title here
key: 409a19bd1e4c9516db4b76c184632c8b

---
## Writing unit tests for Pyspark

```yaml
type: "TitleSlide"
key: "b449e0b160"
```

`@lower_third`

name: Oliver Willekens, PhD
title: Data Engineer at Data Minded


`@script`
Welcome back. In the previous session we've covered different kinds of tests. In this session, we will learn how to write unit tests for our Pyspark application. In doing so, we will restructure our code and create more reusable components.


---
## Our earlier Spark application is a pipeline

```yaml
type: "FullSlide"
key: "f1a248acb4"
disable_transition: false
```

`@part1`
![](https://assets.datacamp.com/production/repositories/4049/datasets/92b879d427d8dd6123f1a1695b25bfbb737f1a8e/etl_00.png)


`@script`
In the previous chapter, we’ve written a simple Spark application to create a more interesting dataset from the source data.

That source data are a bunch of csv files which were on Amazon’s Simple Storage Service, S3. Our data pipeline…


---
## Our earlier Spark application is a pipeline

```yaml
type: "FullSlide"
key: "8ffd270851"
disable_transition: true
```

`@part1`
![](https://assets.datacamp.com/production/repositories/4049/datasets/e490d2be1a5dcfe480776ac80e2af0a22d7af219/etl_01.png)


`@script`
… extracts that data …


---
## Our earlier Spark application is a pipeline

```yaml
type: "FullSlide"
key: "75088ce21f"
disable_transition: true
```

`@part1`
![](https://assets.datacamp.com/production/repositories/4049/datasets/4a87451d0ea07b1af9303939dcb8351b7a6a3290/etl_02.png)


`@script`
… transforms it …


---
## Our earlier Spark application is a pipeline

```yaml
type: "FullSlide"
key: "7a3d89ef33"
disable_transition: true
```

`@part1`
![](https://assets.datacamp.com/production/repositories/4049/datasets/814ca19fe156400a4b3b986b98abbf4f2cb2f78c/etl_03.png)


`@script`
… and finally writes it back to S3.

These 3 steps are often referred to as an “ETL pipeline”.

The first and last step are also the ones where we interact with other services, like databases and file systems. Those interactions are costly, and -as we know from the previous lesson- should be tested, but not as often as our own transformation logic.

The various transformations still need to be fed data somehow, so how do we create Spark dataframes then if they’re not coming from a database or a file?


---
## DataFrames can be constructed in-memory

```yaml
type: "TwoColumns"
key: "d92f360747"
```

`@part1`
```python
# Extract the data
df = spark.read.csv(path)
```{{1}}


`@part2`
```python
record = {'price': 10,
          'quantity': 1, 
          'exchange_rate_to_euro': 2.}
df = spark.createDataFrame([record])
```{{2}}

```python
column_names = ("price", 
                "quantity", 
                "exchange_rate_to_euro")
records = [(10, 1, 2.), (22, 1, 4.3)]
df = spark.createDataFrame(records, 
                           column_names)
```{{3}}


`@script`
Well, Spark DataFrames can be created in-memory too.

In the code we had written earlier, we created a DataFrame by loading a csv file(1). We can replace the creation of simple DataFrames like this(2):
we create a number of dictionaries, and pass a list of these to the `createDataFrame` constructor.

An alternative you will see frequently as well is to separate the column names from the values, like this(3).

There’s a clear benefit here: for testing code, it is convenient to see the data being passed to our functions. We don't have to click around to find or download a file to understand what’s in there.


---
## Create small, reusable and well-named functions

```yaml
type: "FullCodeSlide"
key: "6d6a3051e4"
```

`@part1`
```python
unit_prices_with_ratings = (prices_with_ratings
                            .join(exchange_rates, ["currency", "date"])
                            .withColumn("unit_price_in_euro",
                                        col("price") / col("quantity") 
                                        * col("exchange_rate_to_euro"))
```{{1}}

```python
from functools import partial

def link_with_exchange_rates(prices, rates):
    return prices.join(rates, ["currency", "date"])

def calculate_unit_price_in_euro(df):
    return df.withColumn(
        "unit_price_in_euro",
        col("price") / col("quantity") * col("exchange_rate_to_euro"))
```{{2}}


`@script`
Our original code did a lot at once. This makes it hard to test the functionality.

As an example, this piece was in our original code(1). It linked a dataframe to another and added a column based on some mathematical function.

We could separate these steps like so(2): write out each step to its own function…


---
## Create small, reusable and well-named functions

```yaml
type: "FullCodeSlide"
key: "a1c752ac46"
disable_transition: true
```

`@part1`
```python
unit_prices_with_ratings = (prices_with_ratings
                            .join(exchange_rates, ["currency", "date"])
                            .withColumn("unit_price_in_euro",
                                        col("price") / col("quantity") 
                                        * col("exchange_rate_to_euro"))
```

```python
from functools import partial

def link_with_exchange_rates(prices, rates):
    return prices.join(rates, ["currency", "date"])

def calculate_unit_price_in_euro(df):
    return df.withColumn(
        "unit_price_in_euro",
        col("price") / col("quantity") * col("exchange_rate_to_euro"))

unit_prices_with_ratings = (prices
    .transform(partial(link_with_exchange_rates, exchange_rates))
    .transform(calculate_unit_price_in_euro)
   )
```


`@script`
...and chain them using the `transform` method and partial functions.

While it may seem silly to write a new function for only a single transformation, each transformation by itself can be tested. And reused.


---
## Final Slide

```yaml
type: "FinalSlide"
key: "2ad6d6f053"
```

`@script`


