---
title: Insert title here
key: 409a19bd1e4c9516db4b76c184632c8b

---
## Writing unit tests for Pyspark

```yaml
type: "TitleSlide"
key: "b449e0b160"
```

`@lower_third`

name: Oliver Willekens, PhD
title: Data Engineer at Data Minded


`@script`
Welcome back. In the previous session we've covered different kinds of tests and _why_ good code has tests. In this session, we will learn _how_ to write _unit_ tests for our Pyspark application. In doing so, we will restructure our code and create more reusable components.


---
## Our earlier Spark application is a pipeline

```yaml
type: "FullSlide"
key: "f1a248acb4"
disable_transition: false
```

`@part1`
![](https://assets.datacamp.com/production/repositories/4049/datasets/92b879d427d8dd6123f1a1695b25bfbb737f1a8e/etl_00.png)


`@script`
In the previous chapter, we‚Äôve written a simple Spark application to create a more interesting dataset from source data.

That source data are a bunch of csv files which were on Amazon S3. Our data pipeline‚Ä¶


---
## Our earlier Spark application is a pipeline

```yaml
type: "FullSlide"
key: "8ffd270851"
disable_transition: true
```

`@part1`
![](https://assets.datacamp.com/production/repositories/4049/datasets/e490d2be1a5dcfe480776ac80e2af0a22d7af219/etl_01.png)


`@script`
‚Ä¶ extracts that data ‚Ä¶


---
## Our earlier Spark application is a pipeline

```yaml
type: "FullSlide"
key: "75088ce21f"
disable_transition: true
```

`@part1`
![](https://assets.datacamp.com/production/repositories/4049/datasets/4a87451d0ea07b1af9303939dcb8351b7a6a3290/etl_02.png)


`@script`
‚Ä¶ transforms it ‚Ä¶


---
## Our earlier Spark application is a pipeline

```yaml
type: "FullSlide"
key: "7a3d89ef33"
disable_transition: true
```

`@part1`
![](https://assets.datacamp.com/production/repositories/4049/datasets/814ca19fe156400a4b3b986b98abbf4f2cb2f78c/etl_03.png)


`@script`
‚Ä¶ and finally writes it back to S3.

These 3 steps are often referred to as an ‚ÄúETL pipeline‚Äù.

The first and last steps are also the ones where we interact with other services, like databases and file systems. Those interactions are costly, and -as we know from the previous lesson- should be tested, but not as often as our own transformation logic.

The various transformations still need to be fed data somehow, so how do we create Spark DataFrames then, if they‚Äôre not coming from a database or a file?


---
## Separate transform from extract and load

```yaml
type: "FullSlide"
key: "7b8f3b878f"
```

`@part1`
```python
# Extract
prices_with_ratings = spark.read.csv(...)
exchange_rates = spark.read.csv(...)
...
# Transform
unit_prices_with_ratings = (prices_with_ratings
                            .join(exchange_rates, ["currency", "date"])
                            .withColumn("unit_price_in_euro",
                                        col("price") / col("quantity") 
                                        * col("exchange_rate_to_euro"))
```


`@script`
@Maikel: working on this.
Goal is to show that this piece of code is the one we will write unit tests for. And that there‚Äôs already a first problem because the ‚Äútransformation‚Äù part is not decoupled from the ‚Äúextract"/read, but we still need to feed data somehow to the transform part.


---
## DataFrames can be constructed in-memory

```yaml
type: "TwoRowsTwoColumns"
key: "1d35f655f6"
```

`@part1`
```python
# Extract the data
df = spark.read.csv(path)
```{{1}}


`@part2`
üëé depends on network access{{3}}

üëé unclear how big the data is{{3}}

üëé unclear what data goes in{{3}}


`@part3`
```python
record = {'price': 10,
          'quantity': 1, 
          'exchange_rate_to_euro': 2.}
df = spark.createDataFrame([record])
```{{2}}


`@part4`
üëç inputs are clear{{3}}

üëç data is close to where it is being &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;used (‚Äúcode-proximity‚Äù){{3}}


`@script`
Well, Spark DataFrames can be created in-memory too.

In the code we had written earlier, we created a DataFrame by loading a csv file(1). We can create simple DataFrames like this(2):
create a number of dictionaries, and pass a list of these to the `createDataFrame` constructor.


There‚Äôs a clear benefit here(3): for testing code, it is convenient to see the data being passed to our functions. We don't have to click around to find or download a file to understand what‚Äôs in there.


---
## Create small, reusable and well-named functions

```yaml
type: "FullCodeSlide"
key: "6d6a3051e4"
```

`@part1`
```python
unit_prices_with_ratings = (prices_with_ratings
                            .join(exchange_rates, ["currency", "date"])
                            .withColumn("unit_price_in_euro",
                                        col("price") / col("quantity") 
                                        * col("exchange_rate_to_euro"))
```{{1}}

```python
from functools import partial

def link_with_exchange_rates(prices, rates):
    return prices.join(rates, ["currency", "date"])

def calculate_unit_price_in_euro(df):
    return df.withColumn(
        "unit_price_in_euro",
        col("price") / col("quantity") * col("exchange_rate_to_euro"))
```{{2}}


`@script`
Our original code did a lot at once. This makes it hard to test the functionality.

As an example, this piece was in our original code(1). It linked a dataframe to another and added a column based on some mathematical function.

We could separate these steps like so(2): write out each step to its own function‚Ä¶


---
## Create small, reusable and well-named functions

```yaml
type: "FullCodeSlide"
key: "a1c752ac46"
disable_transition: true
```

`@part1`
```python
unit_prices_with_ratings = (prices_with_ratings
                            .join(exchange_rates, ["currency", "date"])
                            .withColumn("unit_price_in_euro",
                                        col("price") / col("quantity") 
                                        * col("exchange_rate_to_euro"))
```

```python
from functools import partial

def link_with_exchange_rates(prices, rates):
    return prices.join(rates, ["currency", "date"])

def calculate_unit_price_in_euro(df):
    return df.withColumn(
        "unit_price_in_euro",
        col("price") / col("quantity") * col("exchange_rate_to_euro"))

unit_prices_with_ratings = (prices
    .transform(partial(link_with_exchange_rates, exchange_rates))
    .transform(calculate_unit_price_in_euro)
   )
```


`@script`
...and chain them using the `transform` method and partial functions.

While it may seem silly to write a new function for only a single transformation, each transformation by itself can be tested. And reused.


---
## Testing a single unit

```yaml
type: "FullCodeSlide"
key: "9e2b35effe"
```

`@part1`
```python
def test_calculate_unit_price_in_euro(self):
    record = dict(price=10,
                  quantity=5,
                  exchange_rate_to_euro=2.)
    df = self.spark.createDataFrame([record])
```


`@script`
We now know enough to write a test for calculating the unit price in euros. 

We start by creating an in-memory dataframe, with just the columns we need‚Ä¶


---
## Testing a single unit

```yaml
type: "FullCodeSlide"
key: "9df82fb32f"
disable_transition: true
```

`@part1`
```python
def test_calculate_unit_price_in_euro(self):
    record = dict(price=10,
                  quantity=5,
                  exchange_rate_to_euro=2.)
    df = self.spark.createDataFrame([record])
    result = calculate_unit_price_in_euro(df)
```


`@script`
‚Ä¶ We then pass it to the function we want to test‚Ä¶


---
## Testing a single unit

```yaml
type: "FullCodeSlide"
key: "3af64af26a"
disable_transition: true
```

`@part1`
```python
def test_calculate_unit_price_in_euro(self):
    record = dict(price=10,
                  quantity=5,
                  exchange_rate_to_euro=2.)
    df = self.spark.createDataFrame([record])
    result = calculate_unit_price_in_euro(df)

    expected_record = dict(unit_price_in_euro=4., **record)
    expected = self.spark.createDataFrame([expected_record])
```


`@script`
‚Ä¶ Our expectation is that the `unit_price_in_euro` field is added and that the math is correct. We can calculate this example easily ourselves, which is another advantage to running tests with in-memory dataframes.


---
## Testing a single unit

```yaml
type: "FullCodeSlide"
key: "1e4efe6eb4"
disable_transition: true
```

`@part1`
```python
def test_calculate_unit_price_in_euro(self):
    record = dict(price=10,
                  quantity=5,
                  exchange_rate_to_euro=2.)
    df = self.spark.createDataFrame([record])
    result = calculate_unit_price_in_euro(df)

    expected_record = dict(unit_price_in_euro=4., **record)
    expected = self.spark.createDataFrame([expected_record])
    self.assertDataFrameEqual(result, expected)
```


`@script`
‚Ä¶Finally, we must compare the resulting and expected DataFrames. We‚Äôre using a helper function here, `assertDataFrameEqual`, which does exactly that, but its contents are a bit too long for this lesson and you probably have a good notion of what it does.


---
## Take-home messages

```yaml
type: "FullSlide"
key: "e3df51a453"
```

`@part1`
1. Interacting with external data sources is costly{{1}}

2. Creating in-memory DataFrames makes testing easier {{2}}
   * the data is in plain sight,{{2}}
   * focus is on just a small number of examples.{{2}}

3. Creating small and well-named functions leads to more reusability and easier testing.{{3}}


`@script`
A test such as the one we just saw runs well in a testing framework like Pytest. Your testing framework is a preference though. What you should take home from this session is that

1. interacting with external data sources is costly

2. In-memory dataframes allow you to test a single function more easily, by bringing the data in plain sight and focusing on just a small number of examples.

3. Refactor your code so that you create small, reusable and well-named functions.


---
## Let‚Äôs practice!

```yaml
type: "FinalSlide"
key: "2ad6d6f053"
```

`@script`
Now it's your turn to dive into a few exercises, then we'll move forward with automating the testing process.

